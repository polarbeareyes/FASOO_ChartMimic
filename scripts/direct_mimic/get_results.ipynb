{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_path=${YOUR_PROJECT_PATH}\n",
    "project_path=\"/root/SOJUNG_STUFF/ChartMimic_non_changed/ChartMimic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_passed_files(modelagent):\n",
    "    template_type = modelagent.split(\"_\")[-1].split(\"Agent\")[0].lower()\n",
    "    print(template_type)\n",
    "    file_dir = project_path + \"/results/direct/chart2code_{}_results/{}_checker\".format(modelagent, template_type)\n",
    "    filter_files = os.listdir(file_dir)\n",
    "    filter_files = [ item.split(\".pdf\")[0]+\".py\" for item in filter_files if \".pdf\" in item]\n",
    "    print(len(filter_files))\n",
    "    return filter_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [\"gpt-4-vision-preview\", \"claude-3-opus-20240229\", \"gemini-pro-vision\", \"Phi-3-vision-128k-instruct\", \"llava-v1.6-vicuna-7b-hf\", \"deepseek-vl-7b-chat\", \"llava-v1.6-mistral-7b-hf\", \"idefics2-8b\", \"MiniCPM-Llama3-V-2_5\", \"Qwen-VL-Chat\", \"llava-v1.6-vicuna-13b-hf\", \"cogvlm2-llama3-chat-19B\", \"InternVL-Chat-V1-5\", \"llava-v1.6-34b-hf\"]\n",
    "\n",
    "models = [\"deepseek-vl-7b-chat\"]\n",
    "agents = [\"DirectAgent\"]\n",
    "# agents = [\"DirectAgent\", \"HintEnhancedAgent\", \"ScaffoldAgent\", \"SelfRevisionAgent\",]\n",
    "\n",
    "table_type = \"all\" # \"all\". \"prompting\"\n",
    "\n",
    "model_agents = [ \"{}_{}\".format(model, agent) for model in models for agent in agents ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct\n",
      "772\n"
     ]
    }
   ],
   "source": [
    "# filter_type = \"no_filter\" # no_filter, code_pass, chartedit, different_level\n",
    "# filter_type = \"different_level\"\n",
    "filter_type = \"code_pass\"\n",
    "# level = \"hard\"\n",
    "denominator = 1800\n",
    "\n",
    "if filter_type == \"no_filter\":\n",
    "    filter_files_dict = None\n",
    "elif filter_type == \"code_pass\":\n",
    "    filter_files_dict = { model_agent: get_code_passed_files(model_agent) for model_agent in model_agents}\n",
    "else:\n",
    "    raise ValueError(\"filter_type not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a dataframe, \"model\" column is the model name\n",
    "result_df = pd.DataFrame(columns=[\"model_agent\", \"example_count\",\"ExecRate\", \"TextScore\", \"LayoutScore\", \"TypeScore\", \"ColorScore\", \"Average\", \"GPT4VScore\", \"Overall\"])\n",
    "\n",
    "# insert the model name\n",
    "result_df[\"model_agent\"] = [ model + \"_\" + agent for model in models for agent in agents]\n",
    "# set the index to be the model name\n",
    "result_df.set_index(\"model_agent\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: chart2code_deepseek-vl-7b-chat_DirectAgent_results_code4evaluation.json\n",
      "Length of filter files: 772\n",
      "Length of Data: 21\n",
      "Denominator: 600\n",
      "Execution Rate: 1.2866666666666666\n",
      "1.1980880231000004\n",
      "2.6111111111166667\n",
      "1.4806547619000001\n",
      "1.0876754199\n",
      "1.5943823290041668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "for model in models:\n",
    "    for agent in agents:\n",
    "        filename =  project_path + \"/results/direct/chart2code_\" + model + \"_\" + agent +\"_results_code4evaluation.json\"\n",
    "        if os.path.exists(filename):\n",
    "            files.append(filename)\n",
    "        else:\n",
    "            raise FileNotFoundError(\"File not found: {}\".format(filename))\n",
    "\n",
    "for idx, file in enumerate(files):\n",
    "    print(\"Processing file:\", os.path.basename(file))\n",
    "    \n",
    "    data = pd.read_json(file, lines=True)\n",
    "    data[\"orginial\"] = data[\"orginial\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "    data[\"generated\"] = data[\"generated\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "\n",
    "    # filter_files = get_code_passed_files(model_agents[idx])\n",
    "    if filter_files_dict is not None:\n",
    "        filter_files = filter_files_dict[model_agents[idx]]\n",
    "        # print(data[\"orginial\"])\n",
    "        data = data[ data[\"orginial\"].apply(lambda x: any([item == x for item in filter_files])) ]\n",
    "        print(\"Length of filter files:\", len(filter_files))\n",
    "        \n",
    "\n",
    "    print(\"Length of Data:\", len(data))\n",
    "    print(\"Denominator:\", denominator)\n",
    "\n",
    "    f1s = []\n",
    "\n",
    "    result_df.loc[model_agents[idx], \"example_count\"] = len(data)\n",
    "    result_df.loc[model_agents[idx], \"ExecRate\"] = len(filter_files) / denominator\n",
    "    print(\"Execution Rate:\", len(filter_files) / denominator)\n",
    "\n",
    "\n",
    "    text_metrics = data[\"text_metrics\"]\n",
    "    avg_f1 = text_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"TextScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    layout_metrics = data['layout_metrics']\n",
    "    avg_f1 = layout_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"LayoutScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    chart_type_metrics = data[\"chart_type_metrics\"]\n",
    "    avg_f1 = chart_type_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"TypeScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    color_metrics = data[\"color_metrics\"]\n",
    "    avg_f1 = color_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"ColorScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    print( sum(f1s)/len(f1s) )\n",
    "\n",
    "    result_df.loc[model_agents[idx], \"Average\"] = sum(f1s)/len(f1s)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for model in models:\n",
    "    for agent in agents:\n",
    "        file =  project_path + \"/results/direct/chart2code_\" + model + \"_\" + agent +\"_results_gpt4v.json\"\n",
    "        if os.path.exists(file):\n",
    "            files.append(file)\n",
    "\n",
    "\n",
    "for idx, file in enumerate(files):\n",
    "    print(os.path.basename(file))\n",
    "    \n",
    "    data = pd.read_json(file, lines=True)\n",
    "    data[\"orginial\"] = data[\"orginial\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "    data[\"generated\"] = data[\"generated\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "\n",
    "    if filter_files_dict is not None:\n",
    "        filter_files = filter_files_dict[model_agents[idx]]\n",
    "        data = data[ data[\"orginial\"].apply(lambda x: any([item == x for item in filter_files])) ]\n",
    "    print(len(data))\n",
    "\n",
    "\n",
    "    result_df.loc[model_agents[idx], \"example_count\"] = len(data)\n",
    "\n",
    "    gpt4v_score = data[\"gpt4v_score\"]\n",
    "    avg_gpt4v_score = gpt4v_score.sum() / denominator\n",
    "    result_df.loc[model_agents[idx], \"GPT4VScore\"] = avg_gpt4v_score\n",
    "    print(avg_gpt4v_score)\n",
    "\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the overall score\n",
    "result_df[\"Overall\"] = result_df[[\"Average\", \"GPT4VScore\"]].mean(axis=1)\n",
    "result_df[\"ExecRate\"] = result_df[\"ExecRate\"] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_count</th>\n",
       "      <th>ExecRate</th>\n",
       "      <th>TextScore</th>\n",
       "      <th>LayoutScore</th>\n",
       "      <th>TypeScore</th>\n",
       "      <th>ColorScore</th>\n",
       "      <th>Average</th>\n",
       "      <th>GPT4VScore</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_agent</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>deepseek-vl-7b-chat_DirectAgent</th>\n",
       "      <td>21</td>\n",
       "      <td>128.666667</td>\n",
       "      <td>1.198088</td>\n",
       "      <td>2.611111</td>\n",
       "      <td>1.480655</td>\n",
       "      <td>1.087675</td>\n",
       "      <td>1.594382</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.594382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                example_count    ExecRate TextScore  \\\n",
       "model_agent                                                           \n",
       "deepseek-vl-7b-chat_DirectAgent            21  128.666667  1.198088   \n",
       "\n",
       "                                LayoutScore TypeScore ColorScore   Average  \\\n",
       "model_agent                                                                  \n",
       "deepseek-vl-7b-chat_DirectAgent    2.611111  1.480655   1.087675  1.594382   \n",
       "\n",
       "                                GPT4VScore   Overall  \n",
       "model_agent                                           \n",
       "deepseek-vl-7b-chat_DirectAgent        NaN  1.594382  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('results.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chartmimic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
