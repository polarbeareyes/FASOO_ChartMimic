{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_path=${YOUR_PROJECT_PATH}\n",
    "project_path=\"/root/SOJUNG_STUFF/ChartMimic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_passed_files(modelagent):\n",
    "    template_type = modelagent.split(\"_\")[-1].split(\"Agent\")[0].lower()\n",
    "    print(template_type)\n",
    "    file_dir = project_path + \"/results/direct/chart2code_{}_results/{}_checker\".format(modelagent, template_type)\n",
    "    filter_files = os.listdir(file_dir)\n",
    "    filter_files = [ item.split(\".pdf\")[0]+\".py\" for item in filter_files if \".pdf\" in item]\n",
    "    print(len(filter_files))\n",
    "    return filter_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [\"gpt-4-vision-preview\", \"claude-3-opus-20240229\", \"gemini-pro-vision\", \"Phi-3-vision-128k-instruct\", \"llava-v1.6-vicuna-7b-hf\", \"deepseek-vl-7b-chat\", \"llava-v1.6-mistral-7b-hf\", \"idefics2-8b\", \"MiniCPM-Llama3-V-2_5\", \"Qwen-VL-Chat\", \"llava-v1.6-vicuna-13b-hf\", \"cogvlm2-llama3-chat-19B\", \"InternVL-Chat-V1-5\", \"llava-v1.6-34b-hf\"]\n",
    "# deepseek-vl-7b-chat\n",
    "# gemma_nontrained_directcheck\n",
    "# gemma-trained-direct_checker\n",
    "# models = [\"deepseek-vl-7b-chat\"]\n",
    "models = [\"gemma\"]\n",
    "# models = [\"gemma_nontrained_directcheck\"]\n",
    "# models = [\"gemma-trained-direct_checker\"]\n",
    "# models = [\"gemma-trained-v\"]\n",
    "# models = [\"deepseek-vl-7b-chat\", \"gemma_nontrained_directcheck\", \"gemma-trained-direct_checker\"]\n",
    "agents = [\"DirectAgent\"]\n",
    "# agents = [\"DirectAgent\", \"HintEnhancedAgent\", \"ScaffoldAgent\", \"SelfRevisionAgent\",]\n",
    "\n",
    "table_type = \"all\" # \"all\". \"prompting\"\n",
    "\n",
    "model_agents = [ \"{}_{}\".format(model, agent) for model in models for agent in agents ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct\n",
      "1135\n"
     ]
    }
   ],
   "source": [
    "# filter_type = \"no_filter\" # no_filter, code_pass, chartedit, different_level\n",
    "# filter_type = \"different_level\"\n",
    "filter_type = \"code_pass\"\n",
    "# level = \"hard\"\n",
    "denominator = 1800\n",
    "# denominator = 1800\n",
    "\n",
    "if filter_type == \"no_filter\":\n",
    "    filter_files_dict = None\n",
    "elif filter_type == \"code_pass\":\n",
    "    filter_files_dict = { model_agent: get_code_passed_files(model_agent) for model_agent in model_agents}\n",
    "else:\n",
    "    raise ValueError(\"filter_type not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a dataframe, \"model\" column is the model name\n",
    "result_df = pd.DataFrame(columns=[\"model_agent\", \"example_count\",\"ExecRate\", \"TextScore\", \"LayoutScore\", \"TypeScore\", \"ColorScore\", \"Average\", \"GPT4VScore\", \"Overall\"])\n",
    "\n",
    "# insert the model name\n",
    "result_df[\"model_agent\"] = [ model + \"_\" + agent for model in models for agent in agents]\n",
    "# set the index to be the model name\n",
    "result_df.set_index(\"model_agent\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: chart2code_gemma_DirectAgent_results_code4evaluation.json\n",
      "Length of filter files: 1135\n",
      "Length of Data: 891\n",
      "Denominator: 600\n",
      "Execution Rate: 1.8916666666666666\n",
      "95.51633720116666\n",
      "131.96666666676666\n",
      "89.07512795578333\n",
      "78.77657487288333\n",
      "98.83367667414998\n",
      "891\n",
      "Number of rows where all 4 f1 scores are zero: 0\n",
      "Files where all 4 f1 scores are zero: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "for model in models:\n",
    "    for agent in agents:\n",
    "        filename =  project_path + \"/results/direct/chart2code_\" + model + \"_\" + agent +\"_results_code4evaluation.json\"\n",
    "        if os.path.exists(filename):\n",
    "            files.append(filename)\n",
    "        else:\n",
    "            raise FileNotFoundError(\"File not found: {}\".format(filename))\n",
    "\n",
    "for idx, file in enumerate(files):\n",
    "    print(\"Processing file:\", os.path.basename(file))\n",
    "    \n",
    "    data = pd.read_json(file, lines=True)\n",
    "    data[\"orginial\"] = data[\"orginial\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "    data[\"generated\"] = data[\"generated\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "\n",
    "    # filter_files = get_code_passed_files(model_agents[idx])\n",
    "    if filter_files_dict is not None:\n",
    "        filter_files = filter_files_dict[model_agents[idx]]\n",
    "        # print(data[\"orginial\"])\n",
    "        data = data[ data[\"orginial\"].apply(lambda x: any([item == x for item in filter_files])) ]\n",
    "        print(\"Length of filter files:\", len(filter_files))\n",
    "        \n",
    "\n",
    "    print(\"Length of Data:\", len(data))\n",
    "    print(\"Denominator:\", denominator)\n",
    "    #print(\"filter_files:\", len(filter_files)) #debug\n",
    "\n",
    "    f1s = []\n",
    "    \n",
    "\n",
    "    result_df.loc[model_agents[idx], \"example_count\"] = len(data)\n",
    "    result_df.loc[model_agents[idx], \"ExecRate\"] = len(filter_files) / denominator\n",
    "    print(\"Execution Rate:\", len(filter_files) / denominator)\n",
    "\n",
    "\n",
    "    text_metrics = data[\"text_metrics\"]\n",
    "    avg_f1 = text_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"TextScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    layout_metrics = data['layout_metrics']\n",
    "    avg_f1 = layout_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"LayoutScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    chart_type_metrics = data[\"chart_type_metrics\"]\n",
    "    avg_f1 = chart_type_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"TypeScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    color_metrics = data[\"color_metrics\"]\n",
    "    avg_f1 = color_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"ColorScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    print( sum(f1s)/len(f1s) )\n",
    "\n",
    "    result_df.loc[model_agents[idx], \"Average\"] = sum(f1s)/len(f1s)\n",
    "    \n",
    "    zero_f1_rows = data[\n",
    "    (data[\"text_metrics\"].apply(lambda x: x[\"f1\"]) == 0) &\n",
    "    (data[\"layout_metrics\"].apply(lambda x: x[\"f1\"]) == 0) &\n",
    "    (data[\"chart_type_metrics\"].apply(lambda x: x[\"f1\"]) == 0) &\n",
    "    (data[\"color_metrics\"].apply(lambda x: x[\"f1\"]) == 0)\n",
    "    ]\n",
    "\n",
    "    zero_all_f1_count = zero_f1_rows.shape[0]\n",
    "\n",
    "    print(len(data[\"text_metrics\"].apply(lambda x: x[\"f1\"])))\n",
    "    print(\"Number of rows where all 4 f1 scores are zero:\", zero_all_f1_count)\n",
    "\n",
    "    zero_f1_files = zero_f1_rows[\"orginial\"].tolist()\n",
    "    print(\"Files where all 4 f1 scores are zero:\", zero_f1_files)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/root/SOJUNG_STUFF/ChartMimic/highlv_eval/gemma/highlv_results.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m base_org_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/root/SOJUNG_STUFF/ChartMimic/dataset/direct_600/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m base_gen_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/root/SOJUNG_STUFF/ChartMimic/results/direct/chart2code_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m model \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m agent \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_results/direct_checker\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f_in:\n\u001b[1;32m     12\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f_in)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_out:\n",
      "File \u001b[0;32m~/miniconda3/envs/chartmimic310/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/SOJUNG_STUFF/ChartMimic/highlv_eval/gemma/highlv_results.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "print(model)\n",
    "input_path = \"/root/SOJUNG_STUFF/ChartMimic/highlv_eval/\"+ model + \"/highlv_results.json\"\n",
    "output_path = \"/root/SOJUNG_STUFF/ChartMimic/output_converted_\" + model + \".jsonl\"  # 확장자 .jsonl 로 바꾸는 게 관례\n",
    "\n",
    "base_org_dir = \"/root/SOJUNG_STUFF/ChartMimic/dataset/direct_600/\"\n",
    "base_gen_dir = \"/root/SOJUNG_STUFF/ChartMimic/results/direct/chart2code_\"+ model + \"_\" + agent +\"_results/direct_checker\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f_in:\n",
    "    data = json.load(f_in)\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for entry in data:\n",
    "        file_name = entry.get(\"file\")\n",
    "        score = entry.get(\"score\", 0)\n",
    "\n",
    "        original_path_dir = os.path.join(base_org_dir, file_name)\n",
    "        generated_path_dir = os.path.join(base_gen_dir, file_name)\n",
    "\n",
    "        original_path = original_path_dir+ \".py\"\n",
    "        generated_path = generated_path_dir+ \".py\"\n",
    "        reason =  entry.get(\"reason\")\n",
    "        new_entry = {\n",
    "            \"orginial\": original_path,\n",
    "            \"generated\": generated_path,\n",
    "            \"gpt4v_score\": score,\n",
    "            \"reason\": reason\n",
    "        }\n",
    "\n",
    "        json_line = json.dumps(new_entry, ensure_ascii=False)\n",
    "        f_out.write(json_line + \"\\n\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for model in models:\n",
    "    for agent in agents:\n",
    "        # /root/SOJUNG_STUFF/ChartMimic/output_converted_gemma_nontrained_directcheck.json\n",
    "        file =  project_path + \"/output_converted_\" + model + \".jsonl\"\n",
    "        print(file)\n",
    "        if os.path.exists(file):\n",
    "            files.append(file)\n",
    "\n",
    "\n",
    "for idx, file in enumerate(files):\n",
    "    print(os.path.basename(file))\n",
    "    \n",
    "    data = pd.read_json(file, lines=True)\n",
    "    data[\"orginial\"] = data[\"orginial\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "    data[\"generated\"] = data[\"generated\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "\n",
    "    if filter_files_dict is not None:\n",
    "        filter_files = filter_files_dict[model_agents[idx]]\n",
    "        data = data[ data[\"orginial\"].apply(lambda x: any([item == x for item in filter_files])) ]\n",
    "    print(len(data))\n",
    "\n",
    "\n",
    "    result_df.loc[model_agents[idx], \"example_count\"] = len(data)\n",
    "\n",
    "    gpt4v_score = data[\"gpt4v_score\"]\n",
    "    avg_gpt4v_score = gpt4v_score.sum() / denominator\n",
    "    result_df.loc[model_agents[idx], \"GPT4VScore\"] = avg_gpt4v_score\n",
    "    print(avg_gpt4v_score)\n",
    "    \n",
    "    zero_score_count = (gpt4v_score == 0).sum()\n",
    "    print(\"Numbers of gpt4v_score are zero:\", zero_score_count)\n",
    "    zero_score_files = data.loc[data[\"gpt4v_score\"] == 0, \"orginial\"].tolist()\n",
    "    print(\"Files with gpt4v_score == 0:\", zero_score_files)\n",
    "    \n",
    "        # high-level 결과에서 0점인 행만 필터링\n",
    "    zero_score_data = data[data[\"gpt4v_score\"] == 0]\n",
    "\n",
    "    # blank_png 인 파일들만 추출\n",
    "    blank_png_files = zero_score_data[zero_score_data[\"reason\"] == \"blank_png\"][\"orginial\"].tolist()\n",
    "\n",
    "    # 그 외 다른 이유로 0점 받은 파일들\n",
    "    non_blank_png_files = zero_score_data[zero_score_data[\"reason\"] != \"blank_png\"][\"orginial\"].tolist()\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시: 두 모델의 0 f1 점수인 파일 리스트\n",
    "zero_f1_files_model1 = set(zero_f1_files)\n",
    "zero_f1_files_model2 = set(zero_score_files)\n",
    "\n",
    "# 교집합: 두 리스트 모두에 포함된 파일\n",
    "intersection = zero_f1_files_model1 & zero_f1_files_model2\n",
    "\n",
    "# 차집합: 각 리스트에만 포함된 파일\n",
    "only_in_model1 = zero_f1_files_model1 - zero_f1_files_model2\n",
    "only_in_model2 = zero_f1_files_model2 - zero_f1_files_model1\n",
    "\n",
    "print(\"❗겹치는 파일 수 (교집합):\", len(intersection))\n",
    "print(sorted(intersection))\n",
    "\n",
    "print(\"\\n📁low level 에만 있는 파일 수:\", len(only_in_model1))\n",
    "print(sorted(only_in_model1))\n",
    "\n",
    "print(\"\\n📁high level 에만 있는 파일 수:\", len(only_in_model2))\n",
    "print(sorted(only_in_model2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"high level에서 score 0안 파일들 중...\")\n",
    "# 출력\n",
    "print(\"🧊 blank_png reason 파일 수:\", len(blank_png_files))\n",
    "print(sorted(blank_png_files))\n",
    "\n",
    "print(\"\\n🚫 다른 reason 파일 수:\", len(non_blank_png_files))\n",
    "print(sorted(non_blank_png_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_png_files_set = set(\n",
    "    data[data[\"gpt4v_score\"] == 0]\n",
    "    .query(\"reason == 'blank_png'\")[\"orginial\"].tolist()\n",
    ")\n",
    "print(sorted(blank_png_files_set))\n",
    "print(len(blank_png_files_set))\n",
    "\n",
    "\n",
    "files = []\n",
    "for model in models:\n",
    "    for agent in agents:\n",
    "        filename =  project_path + \"/results/direct/chart2code_\" + model + \"_\" + agent +\"_results_code4evaluation.json\"\n",
    "        if os.path.exists(filename):\n",
    "            files.append(filename)\n",
    "        else:\n",
    "            raise FileNotFoundError(\"File not found: {}\".format(filename))\n",
    "\n",
    "for idx, file in enumerate(files):\n",
    "    print(\"Processing file:\", os.path.basename(file))\n",
    "    \n",
    "    data = pd.read_json(file, lines=True)\n",
    "    data[\"orginial\"] = data[\"orginial\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "    data[\"generated\"] = data[\"generated\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "    \n",
    "    if 'blank_png_files_set' in globals():  # 혹시 외부에서 가져온 경우\n",
    "        def overwrite_f1_if_blank(row):\n",
    "            if row[\"orginial\"] in blank_png_files_set:\n",
    "                row[\"text_metrics\"][\"f1\"] = 0\n",
    "                row[\"layout_metrics\"][\"f1\"] = 0\n",
    "                row[\"chart_type_metrics\"][\"f1\"] = 0\n",
    "                row[\"color_metrics\"][\"f1\"] = 0\n",
    "            return row\n",
    "        data = data.apply(overwrite_f1_if_blank, axis=1)\n",
    "\n",
    "    # filter_files = get_code_passed_files(model_agents[idx])\n",
    "    if filter_files_dict is not None:\n",
    "        filter_files = filter_files_dict[model_agents[idx]]\n",
    "        # print(data[\"orginial\"])\n",
    "        data = data[ data[\"orginial\"].apply(lambda x: any([item == x for item in filter_files])) ]\n",
    "        print(\"Length of filter files:\", len(filter_files))\n",
    "        \n",
    "\n",
    "    print(\"Length of Data:\", len(data))\n",
    "    print(\"Denominator:\", denominator)\n",
    "    #print(\"filter_files:\", len(filter_files)) #debug\n",
    "\n",
    "    f1s = []\n",
    "    \n",
    "\n",
    "    result_df.loc[model_agents[idx], \"example_count\"] = len(data)\n",
    "    result_df.loc[model_agents[idx], \"ExecRate\"] = len(filter_files) / denominator\n",
    "    print(\"Execution Rate:\", len(filter_files) / denominator)\n",
    "\n",
    "\n",
    "    text_metrics = data[\"text_metrics\"]\n",
    "    avg_f1 = text_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"TextScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    layout_metrics = data['layout_metrics']\n",
    "    avg_f1 = layout_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"LayoutScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    chart_type_metrics = data[\"chart_type_metrics\"]\n",
    "    avg_f1 = chart_type_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"TypeScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    color_metrics = data[\"color_metrics\"]\n",
    "    avg_f1 = color_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"ColorScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    print( sum(f1s)/len(f1s) )\n",
    "\n",
    "    result_df.loc[model_agents[idx], \"Average\"] = sum(f1s)/len(f1s)\n",
    "    \n",
    "    zero_f1_rows = data[\n",
    "    (data[\"text_metrics\"].apply(lambda x: x[\"f1\"]) == 0) &\n",
    "    (data[\"layout_metrics\"].apply(lambda x: x[\"f1\"]) == 0) &\n",
    "    (data[\"chart_type_metrics\"].apply(lambda x: x[\"f1\"]) == 0) &\n",
    "    (data[\"color_metrics\"].apply(lambda x: x[\"f1\"]) == 0)\n",
    "    ]\n",
    "\n",
    "    zero_all_f1_count = zero_f1_rows.shape[0]\n",
    "\n",
    "    print(len(data[\"text_metrics\"].apply(lambda x: x[\"f1\"])))\n",
    "    print(\"Number of rows where all 4 f1 scores are zero:\", zero_all_f1_count)\n",
    "\n",
    "    zero_f1_files = zero_f1_rows[\"orginial\"].tolist()\n",
    "    print(\"Files where all 4 f1 scores are zero:\", zero_f1_files)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the overall score\n",
    "result_df[\"Overall\"] = result_df[[\"Average\", \"GPT4VScore\"]].mean(axis=1)\n",
    "result_df[\"ExecRate\"] = result_df[\"ExecRate\"] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('results.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chartmimic310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
