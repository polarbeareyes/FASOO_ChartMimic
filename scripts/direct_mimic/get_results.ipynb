{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_path=${YOUR_PROJECT_PATH}\n",
    "project_path=\"/root/SOJUNG_STUFF/ChartMimic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_passed_files(modelagent):\n",
    "    template_type = modelagent.split(\"_\")[-1].split(\"Agent\")[0].lower()\n",
    "    print(template_type)\n",
    "    file_dir = project_path + \"/results/direct/chart2code_{}_results/{}_checker\".format(modelagent, template_type)\n",
    "    filter_files = os.listdir(file_dir)\n",
    "    filter_files = [ item.split(\".pdf\")[0]+\".py\" for item in filter_files if \".pdf\" in item]\n",
    "    print(len(filter_files))\n",
    "    return filter_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [\"gpt-4-vision-preview\", \"claude-3-opus-20240229\", \"gemini-pro-vision\", \"Phi-3-vision-128k-instruct\", \"llava-v1.6-vicuna-7b-hf\", \"deepseek-vl-7b-chat\", \"llava-v1.6-mistral-7b-hf\", \"idefics2-8b\", \"MiniCPM-Llama3-V-2_5\", \"Qwen-VL-Chat\", \"llava-v1.6-vicuna-13b-hf\", \"cogvlm2-llama3-chat-19B\", \"InternVL-Chat-V1-5\", \"llava-v1.6-34b-hf\"]\n",
    "# deepseek-vl-7b-chat\n",
    "# gemma_nontrained_directcheck\n",
    "# gemma-trained-direct_checker\n",
    "models = [\"deepseek-vl-7b-chat\"]\n",
    "# models = [\"gemma_nontrained_directcheck\"]\n",
    "# models = [\"gemma-trained-direct_checker\"]\n",
    "# models = [\"gemma-trained-v\"]\n",
    "# models = [\"deepseek-vl-7b-chat\", \"gemma_nontrained_directcheck\", \"gemma-trained-direct_checker\"]\n",
    "agents = [\"DirectAgent\"]\n",
    "# agents = [\"DirectAgent\", \"HintEnhancedAgent\", \"ScaffoldAgent\", \"SelfRevisionAgent\",]\n",
    "\n",
    "table_type = \"all\" # \"all\". \"prompting\"\n",
    "\n",
    "model_agents = [ \"{}_{}\".format(model, agent) for model in models for agent in agents ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct\n",
      "219\n"
     ]
    }
   ],
   "source": [
    "# filter_type = \"no_filter\" # no_filter, code_pass, chartedit, different_level\n",
    "# filter_type = \"different_level\"\n",
    "filter_type = \"code_pass\"\n",
    "# level = \"hard\"\n",
    "denominator = 600\n",
    "# denominator = 1800\n",
    "\n",
    "if filter_type == \"no_filter\":\n",
    "    filter_files_dict = None\n",
    "elif filter_type == \"code_pass\":\n",
    "    filter_files_dict = { model_agent: get_code_passed_files(model_agent) for model_agent in model_agents}\n",
    "else:\n",
    "    raise ValueError(\"filter_type not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a dataframe, \"model\" column is the model name\n",
    "result_df = pd.DataFrame(columns=[\"model_agent\", \"example_count\",\"ExecRate\", \"TextScore\", \"LayoutScore\", \"TypeScore\", \"ColorScore\", \"Average\", \"GPT4VScore\", \"Overall\"])\n",
    "\n",
    "# insert the model name\n",
    "result_df[\"model_agent\"] = [ model + \"_\" + agent for model in models for agent in agents]\n",
    "# set the index to be the model name\n",
    "result_df.set_index(\"model_agent\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: chart2code_deepseek-vl-7b-chat_DirectAgent_results_code4evaluation.json\n",
      "Length of filter files: 219\n",
      "Length of Data: 221\n",
      "Denominator: 600\n",
      "Execution Rate: 0.365\n",
      "15.625146229783335\n",
      "27.40714285715\n",
      "18.43381358565\n",
      "16.289826707366668\n",
      "19.4389823449875\n",
      "221\n",
      "Number of rows where all 4 f1 scores are zero: 7\n",
      "Files where all 4 f1 scores are zero: ['density_4.py', 'area_18.py', '3d_2.py', 'heatmap_23.py', 'contour_8.py', 'hist_17.py', 'heatmap_22.py']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "for model in models:\n",
    "    for agent in agents:\n",
    "        filename =  project_path + \"/results/direct/chart2code_\" + model + \"_\" + agent +\"_results_code4evaluation.json\"\n",
    "        if os.path.exists(filename):\n",
    "            files.append(filename)\n",
    "        else:\n",
    "            raise FileNotFoundError(\"File not found: {}\".format(filename))\n",
    "\n",
    "for idx, file in enumerate(files):\n",
    "    print(\"Processing file:\", os.path.basename(file))\n",
    "    \n",
    "    data = pd.read_json(file, lines=True)\n",
    "    data[\"orginial\"] = data[\"orginial\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "    data[\"generated\"] = data[\"generated\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "\n",
    "    # filter_files = get_code_passed_files(model_agents[idx])\n",
    "    if filter_files_dict is not None:\n",
    "        filter_files = filter_files_dict[model_agents[idx]]\n",
    "        # print(data[\"orginial\"])\n",
    "        data = data[ data[\"orginial\"].apply(lambda x: any([item == x for item in filter_files])) ]\n",
    "        print(\"Length of filter files:\", len(filter_files))\n",
    "        \n",
    "\n",
    "    print(\"Length of Data:\", len(data))\n",
    "    print(\"Denominator:\", denominator)\n",
    "    #print(\"filter_files:\", len(filter_files)) #debug\n",
    "\n",
    "    f1s = []\n",
    "    \n",
    "\n",
    "    result_df.loc[model_agents[idx], \"example_count\"] = len(data)\n",
    "    result_df.loc[model_agents[idx], \"ExecRate\"] = len(filter_files) / denominator\n",
    "    print(\"Execution Rate:\", len(filter_files) / denominator)\n",
    "\n",
    "\n",
    "    text_metrics = data[\"text_metrics\"]\n",
    "    avg_f1 = text_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"TextScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    layout_metrics = data['layout_metrics']\n",
    "    avg_f1 = layout_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"LayoutScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    chart_type_metrics = data[\"chart_type_metrics\"]\n",
    "    avg_f1 = chart_type_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"TypeScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    color_metrics = data[\"color_metrics\"]\n",
    "    avg_f1 = color_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"ColorScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    print( sum(f1s)/len(f1s) )\n",
    "\n",
    "    result_df.loc[model_agents[idx], \"Average\"] = sum(f1s)/len(f1s)\n",
    "    \n",
    "    zero_f1_rows = data[\n",
    "    (data[\"text_metrics\"].apply(lambda x: x[\"f1\"]) == 0) &\n",
    "    (data[\"layout_metrics\"].apply(lambda x: x[\"f1\"]) == 0) &\n",
    "    (data[\"chart_type_metrics\"].apply(lambda x: x[\"f1\"]) == 0) &\n",
    "    (data[\"color_metrics\"].apply(lambda x: x[\"f1\"]) == 0)\n",
    "    ]\n",
    "\n",
    "    zero_all_f1_count = zero_f1_rows.shape[0]\n",
    "\n",
    "    print(len(data[\"text_metrics\"].apply(lambda x: x[\"f1\"])))\n",
    "    print(\"Number of rows where all 4 f1 scores are zero:\", zero_all_f1_count)\n",
    "\n",
    "    zero_f1_files = zero_f1_rows[\"orginial\"].tolist()\n",
    "    print(\"Files where all 4 f1 scores are zero:\", zero_f1_files)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek-vl-7b-chat\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "print(model)\n",
    "input_path = \"/root/SOJUNG_STUFF/ChartMimic/highlv_eval/\"+ model + \"/highlv_results.json\"\n",
    "output_path = \"/root/SOJUNG_STUFF/ChartMimic/output_converted_\" + model + \".jsonl\"  # í™•ì¥ì .jsonl ë¡œ ë°”ê¾¸ëŠ” ê²Œ ê´€ë¡€\n",
    "\n",
    "base_org_dir = \"/root/SOJUNG_STUFF/ChartMimic/dataset/direct_600/\"\n",
    "base_gen_dir = \"/root/SOJUNG_STUFF/ChartMimic/results/direct/chart2code_\"+ model + \"_\" + agent +\"_results/direct_checker\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f_in:\n",
    "    data = json.load(f_in)\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for entry in data:\n",
    "        file_name = entry.get(\"file\")\n",
    "        score = entry.get(\"score\", 0)\n",
    "\n",
    "        original_path_dir = os.path.join(base_org_dir, file_name)\n",
    "        generated_path_dir = os.path.join(base_gen_dir, file_name)\n",
    "\n",
    "        original_path = original_path_dir+ \".py\"\n",
    "        generated_path = generated_path_dir+ \".py\"\n",
    "        reason =  entry.get(\"reason\")\n",
    "        new_entry = {\n",
    "            \"orginial\": original_path,\n",
    "            \"generated\": generated_path,\n",
    "            \"gpt4v_score\": score,\n",
    "            \"reason\": reason\n",
    "        }\n",
    "\n",
    "        json_line = json.dumps(new_entry, ensure_ascii=False)\n",
    "        f_out.write(json_line + \"\\n\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/SOJUNG_STUFF/ChartMimic/output_converted_deepseek-vl-7b-chat.jsonl\n",
      "output_converted_deepseek-vl-7b-chat.jsonl\n",
      "219\n",
      "11.498333333333333\n",
      "Numbers of gpt4v_score are zero: 18\n",
      "Files with gpt4v_score == 0: ['HR_1.py', 'multidiff_2.py', 'hist_5.py', 'line_12.py', 'graph_13.py', 'PIP_7.py', 'pie_11.py', 'scatter_9.py', 'line_10.py', 'hist_17.py', 'line_7.py', 'density_4.py', 'errorpoint_18.py', 'CB_7.py', 'CB_19.py', 'multidiff_23.py', 'errorpoint_2.py', 'contour_8.py']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "for model in models:\n",
    "    for agent in agents:\n",
    "        # /root/SOJUNG_STUFF/ChartMimic/output_converted_gemma_nontrained_directcheck.json\n",
    "        file =  project_path + \"/output_converted_\" + model + \".jsonl\"\n",
    "        print(file)\n",
    "        if os.path.exists(file):\n",
    "            files.append(file)\n",
    "\n",
    "\n",
    "for idx, file in enumerate(files):\n",
    "    print(os.path.basename(file))\n",
    "    \n",
    "    data = pd.read_json(file, lines=True)\n",
    "    data[\"orginial\"] = data[\"orginial\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "    data[\"generated\"] = data[\"generated\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "\n",
    "    if filter_files_dict is not None:\n",
    "        filter_files = filter_files_dict[model_agents[idx]]\n",
    "        data = data[ data[\"orginial\"].apply(lambda x: any([item == x for item in filter_files])) ]\n",
    "    print(len(data))\n",
    "\n",
    "\n",
    "    result_df.loc[model_agents[idx], \"example_count\"] = len(data)\n",
    "\n",
    "    gpt4v_score = data[\"gpt4v_score\"]\n",
    "    avg_gpt4v_score = gpt4v_score.sum() / denominator\n",
    "    result_df.loc[model_agents[idx], \"GPT4VScore\"] = avg_gpt4v_score\n",
    "    print(avg_gpt4v_score)\n",
    "    \n",
    "    zero_score_count = (gpt4v_score == 0).sum()\n",
    "    print(\"Numbers of gpt4v_score are zero:\", zero_score_count)\n",
    "    zero_score_files = data.loc[data[\"gpt4v_score\"] == 0, \"orginial\"].tolist()\n",
    "    print(\"Files with gpt4v_score == 0:\", zero_score_files)\n",
    "    \n",
    "        # high-level ê²°ê³¼ì—ì„œ 0ì ì¸ í–‰ë§Œ í•„í„°ë§\n",
    "    zero_score_data = data[data[\"gpt4v_score\"] == 0]\n",
    "\n",
    "    # blank_png ì¸ íŒŒì¼ë“¤ë§Œ ì¶”ì¶œ\n",
    "    blank_png_files = zero_score_data[zero_score_data[\"reason\"] == \"blank_png\"][\"orginial\"].tolist()\n",
    "\n",
    "    # ê·¸ ì™¸ ë‹¤ë¥¸ ì´ìœ ë¡œ 0ì  ë°›ì€ íŒŒì¼ë“¤\n",
    "    non_blank_png_files = zero_score_data[zero_score_data[\"reason\"] != \"blank_png\"][\"orginial\"].tolist()\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â—ê²¹ì¹˜ëŠ” íŒŒì¼ ìˆ˜ (êµì§‘í•©): 3\n",
      "['contour_8.py', 'density_4.py', 'hist_17.py']\n",
      "\n",
      "ğŸ“low level ì—ë§Œ ìˆëŠ” íŒŒì¼ ìˆ˜: 4\n",
      "['3d_2.py', 'area_18.py', 'heatmap_22.py', 'heatmap_23.py']\n",
      "\n",
      "ğŸ“high level ì—ë§Œ ìˆëŠ” íŒŒì¼ ìˆ˜: 15\n",
      "['CB_19.py', 'CB_7.py', 'HR_1.py', 'PIP_7.py', 'errorpoint_18.py', 'errorpoint_2.py', 'graph_13.py', 'hist_5.py', 'line_10.py', 'line_12.py', 'line_7.py', 'multidiff_2.py', 'multidiff_23.py', 'pie_11.py', 'scatter_9.py']\n"
     ]
    }
   ],
   "source": [
    "# ì˜ˆì‹œ: ë‘ ëª¨ë¸ì˜ 0 f1 ì ìˆ˜ì¸ íŒŒì¼ ë¦¬ìŠ¤íŠ¸\n",
    "zero_f1_files_model1 = set(zero_f1_files)\n",
    "zero_f1_files_model2 = set(zero_score_files)\n",
    "\n",
    "# êµì§‘í•©: ë‘ ë¦¬ìŠ¤íŠ¸ ëª¨ë‘ì— í¬í•¨ëœ íŒŒì¼\n",
    "intersection = zero_f1_files_model1 & zero_f1_files_model2\n",
    "\n",
    "# ì°¨ì§‘í•©: ê° ë¦¬ìŠ¤íŠ¸ì—ë§Œ í¬í•¨ëœ íŒŒì¼\n",
    "only_in_model1 = zero_f1_files_model1 - zero_f1_files_model2\n",
    "only_in_model2 = zero_f1_files_model2 - zero_f1_files_model1\n",
    "\n",
    "print(\"â—ê²¹ì¹˜ëŠ” íŒŒì¼ ìˆ˜ (êµì§‘í•©):\", len(intersection))\n",
    "print(sorted(intersection))\n",
    "\n",
    "print(\"\\nğŸ“low level ì—ë§Œ ìˆëŠ” íŒŒì¼ ìˆ˜:\", len(only_in_model1))\n",
    "print(sorted(only_in_model1))\n",
    "\n",
    "print(\"\\nğŸ“high level ì—ë§Œ ìˆëŠ” íŒŒì¼ ìˆ˜:\", len(only_in_model2))\n",
    "print(sorted(only_in_model2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high levelì—ì„œ score 0ì•ˆ íŒŒì¼ë“¤ ì¤‘...\n",
      "ğŸ§Š blank_png reason íŒŒì¼ ìˆ˜: 17\n",
      "['CB_19.py', 'CB_7.py', 'HR_1.py', 'PIP_7.py', 'contour_8.py', 'density_4.py', 'errorpoint_18.py', 'errorpoint_2.py', 'graph_13.py', 'hist_5.py', 'line_10.py', 'line_12.py', 'line_7.py', 'multidiff_2.py', 'multidiff_23.py', 'pie_11.py', 'scatter_9.py']\n",
      "\n",
      "ğŸš« ë‹¤ë¥¸ reason íŒŒì¼ ìˆ˜: 1\n",
      "['hist_17.py']\n"
     ]
    }
   ],
   "source": [
    "print(\"high levelì—ì„œ score 0ì•ˆ íŒŒì¼ë“¤ ì¤‘...\")\n",
    "# ì¶œë ¥\n",
    "print(\"ğŸ§Š blank_png reason íŒŒì¼ ìˆ˜:\", len(blank_png_files))\n",
    "print(sorted(blank_png_files))\n",
    "\n",
    "print(\"\\nğŸš« ë‹¤ë¥¸ reason íŒŒì¼ ìˆ˜:\", len(non_blank_png_files))\n",
    "print(sorted(non_blank_png_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CB_19.py', 'CB_7.py', 'HR_1.py', 'PIP_7.py', 'contour_8.py', 'density_4.py', 'errorpoint_18.py', 'errorpoint_2.py', 'graph_13.py', 'hist_5.py', 'line_10.py', 'line_12.py', 'line_7.py', 'multidiff_2.py', 'multidiff_23.py', 'pie_11.py', 'scatter_9.py']\n",
      "17\n",
      "Processing file: chart2code_deepseek-vl-7b-chat_DirectAgent_results_code4evaluation.json\n",
      "Length of filter files: 219\n",
      "Length of Data: 221\n",
      "Denominator: 600\n",
      "Execution Rate: 0.365\n",
      "15.625146229783335\n",
      "27.40714285715\n",
      "16.78841209171667\n",
      "14.974552158700002\n",
      "18.6988133343375\n",
      "221\n",
      "Number of rows where all 4 f1 scores are zero: 22\n",
      "Files where all 4 f1 scores are zero: ['density_4.py', 'area_18.py', 'scatter_9.py', '3d_2.py', 'heatmap_23.py', 'contour_8.py', 'line_12.py', 'graph_13.py', 'line_10.py', 'CB_7.py', 'CB_19.py', 'pie_11.py', 'hist_17.py', 'multidiff_2.py', 'multidiff_23.py', 'errorpoint_2.py', 'PIP_7.py', 'hist_5.py', 'HR_1.py', 'errorpoint_18.py', 'heatmap_22.py', 'line_7.py']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blank_png_files_set = set(\n",
    "    data[data[\"gpt4v_score\"] == 0]\n",
    "    .query(\"reason == 'blank_png'\")[\"orginial\"].tolist()\n",
    ")\n",
    "print(sorted(blank_png_files_set))\n",
    "print(len(blank_png_files_set))\n",
    "\n",
    "\n",
    "files = []\n",
    "for model in models:\n",
    "    for agent in agents:\n",
    "        filename =  project_path + \"/results/direct/chart2code_\" + model + \"_\" + agent +\"_results_code4evaluation.json\"\n",
    "        if os.path.exists(filename):\n",
    "            files.append(filename)\n",
    "        else:\n",
    "            raise FileNotFoundError(\"File not found: {}\".format(filename))\n",
    "\n",
    "for idx, file in enumerate(files):\n",
    "    print(\"Processing file:\", os.path.basename(file))\n",
    "    \n",
    "    data = pd.read_json(file, lines=True)\n",
    "    data[\"orginial\"] = data[\"orginial\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "    data[\"generated\"] = data[\"generated\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "    \n",
    "    if 'blank_png_files_set' in globals():  # í˜¹ì‹œ ì™¸ë¶€ì—ì„œ ê°€ì ¸ì˜¨ ê²½ìš°\n",
    "        def overwrite_f1_if_blank(row):\n",
    "            if row[\"orginial\"] in blank_png_files_set:\n",
    "                row[\"text_metrics\"][\"f1\"] = 0\n",
    "                row[\"layout_metrics\"][\"f1\"] = 0\n",
    "                row[\"chart_type_metrics\"][\"f1\"] = 0\n",
    "                row[\"color_metrics\"][\"f1\"] = 0\n",
    "            return row\n",
    "        data = data.apply(overwrite_f1_if_blank, axis=1)\n",
    "\n",
    "    # filter_files = get_code_passed_files(model_agents[idx])\n",
    "    if filter_files_dict is not None:\n",
    "        filter_files = filter_files_dict[model_agents[idx]]\n",
    "        # print(data[\"orginial\"])\n",
    "        data = data[ data[\"orginial\"].apply(lambda x: any([item == x for item in filter_files])) ]\n",
    "        print(\"Length of filter files:\", len(filter_files))\n",
    "        \n",
    "\n",
    "    print(\"Length of Data:\", len(data))\n",
    "    print(\"Denominator:\", denominator)\n",
    "    #print(\"filter_files:\", len(filter_files)) #debug\n",
    "\n",
    "    f1s = []\n",
    "    \n",
    "\n",
    "    result_df.loc[model_agents[idx], \"example_count\"] = len(data)\n",
    "    result_df.loc[model_agents[idx], \"ExecRate\"] = len(filter_files) / denominator\n",
    "    print(\"Execution Rate:\", len(filter_files) / denominator)\n",
    "\n",
    "\n",
    "    text_metrics = data[\"text_metrics\"]\n",
    "    avg_f1 = text_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"TextScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    layout_metrics = data['layout_metrics']\n",
    "    avg_f1 = layout_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"LayoutScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    chart_type_metrics = data[\"chart_type_metrics\"]\n",
    "    avg_f1 = chart_type_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"TypeScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    color_metrics = data[\"color_metrics\"]\n",
    "    avg_f1 = color_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"ColorScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    print( sum(f1s)/len(f1s) )\n",
    "\n",
    "    result_df.loc[model_agents[idx], \"Average\"] = sum(f1s)/len(f1s)\n",
    "    \n",
    "    zero_f1_rows = data[\n",
    "    (data[\"text_metrics\"].apply(lambda x: x[\"f1\"]) == 0) &\n",
    "    (data[\"layout_metrics\"].apply(lambda x: x[\"f1\"]) == 0) &\n",
    "    (data[\"chart_type_metrics\"].apply(lambda x: x[\"f1\"]) == 0) &\n",
    "    (data[\"color_metrics\"].apply(lambda x: x[\"f1\"]) == 0)\n",
    "    ]\n",
    "\n",
    "    zero_all_f1_count = zero_f1_rows.shape[0]\n",
    "\n",
    "    print(len(data[\"text_metrics\"].apply(lambda x: x[\"f1\"])))\n",
    "    print(\"Number of rows where all 4 f1 scores are zero:\", zero_all_f1_count)\n",
    "\n",
    "    zero_f1_files = zero_f1_rows[\"orginial\"].tolist()\n",
    "    print(\"Files where all 4 f1 scores are zero:\", zero_f1_files)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the overall score\n",
    "result_df[\"Overall\"] = result_df[[\"Average\", \"GPT4VScore\"]].mean(axis=1)\n",
    "result_df[\"ExecRate\"] = result_df[\"ExecRate\"] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_count</th>\n",
       "      <th>ExecRate</th>\n",
       "      <th>TextScore</th>\n",
       "      <th>LayoutScore</th>\n",
       "      <th>TypeScore</th>\n",
       "      <th>ColorScore</th>\n",
       "      <th>Average</th>\n",
       "      <th>GPT4VScore</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_agent</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>deepseek-vl-7b-chat_DirectAgent</th>\n",
       "      <td>221</td>\n",
       "      <td>36.5</td>\n",
       "      <td>15.625146</td>\n",
       "      <td>27.407143</td>\n",
       "      <td>16.788412</td>\n",
       "      <td>14.974552</td>\n",
       "      <td>18.698813</td>\n",
       "      <td>11.498333</td>\n",
       "      <td>15.098573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                example_count ExecRate  TextScore LayoutScore  \\\n",
       "model_agent                                                                     \n",
       "deepseek-vl-7b-chat_DirectAgent           221     36.5  15.625146   27.407143   \n",
       "\n",
       "                                 TypeScore ColorScore    Average GPT4VScore  \\\n",
       "model_agent                                                                   \n",
       "deepseek-vl-7b-chat_DirectAgent  16.788412  14.974552  18.698813  11.498333   \n",
       "\n",
       "                                   Overall  \n",
       "model_agent                                 \n",
       "deepseek-vl-7b-chat_DirectAgent  15.098573  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('results.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chartmimic310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
