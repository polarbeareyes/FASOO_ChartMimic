import os
import json
from tqdm import tqdm
from PIL import Image
from pdf2image import convert_from_path
import base64
from dotenv import load_dotenv
from openai import OpenAI
load_dotenv()
client = OpenAI()

class Evaluator:
    def __init__(self, original_dataset_dir, generated_dataset_dir, results_dir):
        self.original_dir = original_dataset_dir
        self.generated_dir = generated_dataset_dir
        self.results_dir = results_dir
        os.makedirs(results_dir, exist_ok=True)

        self.raw_data = self._get_all_files(self.original_dir, ".py")
        self.results = []
        self.no_py_generated = []
        self.no_pdf_generated = []
        self.corrupted_pdf_files = []

        self._load_data()

    def _get_all_files(self, dataset_dir, file_type):
        selected_files = []
        for root, _, files in os.walk(dataset_dir):
            for file in files:
                if file.endswith(file_type):
                    selected_files.append(os.path.join(root, file))
        return selected_files

    def _convert_single_page_pdf_to_png(self, pdf_path, output_path, dpi=350):
        try:
            images = convert_from_path(pdf_path, dpi=dpi)
            images[0].save(output_path, "PNG")
            return True
        except Exception as e: 
            print(f"[WARNING] PDF 변환 실패: {pdf_path} — {e}") #added after getting pdfinfo Value error
            return False
        
    def _is_png_white(self, png_path):
        try:
            img = Image.open(png_path).convert("RGB")
            return all(pixel == (255, 255, 255) for pixel in img.getdata())
        except:
            return True

    def _build_result_entry(self, base_name, score, reason=None):
        return {
            "file": base_name,
            "score": score,
            "reason": reason
        }
    
    def _encode_image(self, path):
        with open(path, "rb") as img_file:
            return base64.b64encode(img_file.read()).decode("utf-8")


    def _load_data(self):
        for data in tqdm(self.raw_data, desc="Evaluating"):
            base_name = os.path.splitext(os.path.basename(data))[0]
            gen_pdf = os.path.join(self.generated_dir, base_name + ".pdf")
            gen_png = os.path.join(self.generated_dir, base_name + ".png")
            gen_py = os.path.join(self.generated_dir, base_name + ".py")
            if not os.path.exists(gen_py):
                self.no_py_generated.append(base_name)
                self.results.append(self._build_result_entry(base_name, 0, reason="no_py"))
                continue
            if not os.path.exists(gen_pdf):
                self.no_pdf_generated.append(base_name)
                self.results.append(self._build_result_entry(base_name, 0, reason="no_pdf"))
                continue
            if not os.path.exists(gen_png):
                success = self._convert_single_page_pdf_to_png(gen_pdf, gen_png)
                if not success:
                    self.corrupted_pdf_files.append(base_name)

                    self.results.append({
                        "file": base_name,
                        "score": 0.0,  
                        "comment": None,
                        "reason": "corrupted_pdf"  
                    })
                    continue

            if self._is_png_white(gen_png):
                self.results.append(self._build_result_entry(base_name, 0, reason="blank_png"))
                continue

            eval_result = self._evaluate(
                os.path.join(self.original_dir, base_name + ".png"),
                gen_png
            )
            self.results.append({
                "file": base_name,
                "score": eval_result["score"],
                "comment": eval_result["comment"],
                "reason": None
            })

        self._save_results()
    
    def _evaluate(self, org_png, gen_png):
        prompt = """
        "You are an excellent judge at evaluating visualization chart plots. 
        The first image (reference image) is created using ground truth matplotlib code, and the second image (AI-generated image) is created using matplotlib code generated by an AI assistant. 
        Your task is to score how well the AI-generated plot matches the ground truth plot.
        \n\n### Scoring Methodology:\n
        The AI-generated image's score is based on the following criteria, totaling a score out of 100 points:\n\n
        1. **Chart Types (20 points)** Does the AI-generated image include all chart types present in the reference image (e.g., line charts, bar charts, etc.)?\n
        2. **Layout (10 points)** Does the arrangement of subplots in the AI-generated image match the reference image (e.g., number of rows and columns)?\n
        3. **Text Content (20 points)** Does the AI-generated image include all text from the reference image (e.g., titles, annotations, axis labels), excluding axis tick labels?\n
        4. **Data (20 points)** How accurately do the data trends in the AI-generated image resemble those in the original image and is the number of data groups the same as in the reference image?\n
        5. **Style (20 points)** Does the AI-generated image match the original in terms of colors (line colors, fill colors, etc.), marker types (point shapes, line styles, etc.), legends, grids, and other stylistic details?\n
        6. **Clarity (10 points)** Is the AI-generated image clear and free of overlapping elements?\n\n
        ### Evaluation:\n
        Compare the two images head to head and provide a detailed assessment. Use the following format for your response:\n\n\n
        ---\n\n
        Comments:\n
        - Chart Types: ${your comment and subscore}\n
        - Layout: ${your comment and subscore}\n
        - Text Content: ${your comment and subscore}\n
        - Data: ${your comment and subscore}\n
        - Style: ${your comment and subscore}\n
        - Clarity: ${your comment and subscore}\n\n
        Score: ${your final score out of 100}\n\n
        ---\n\n
        Please use the above format to ensure the evaluation is clear and comprehensive.\n"
        """
        

        
        orig_b64 = self._encode_image(org_png)
        gen_b64 = self._encode_image(gen_png)

        try:
            response = client.chat.completions.create(
                model="o4-mini",
                messages=[
                    {"role": "user", "content": prompt},
                    {"role": "user", "content": [
                        {"type": "text", "text": "This is the original image."},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{orig_b64}"
                            }
                        },
                        {"type": "text", "text": "This is the ai-generated image."},
                        {
                            "type": "image_url",
                            "image_url":{
                                    "url": f"data:image/png;base64,{gen_b64}"
                                }
                        }
                    ]}
                ],
                max_completion_tokens=4096,
            )
            print(response)
            result_text = response.choices[0].message.content
            import re
            score_match = re.search(r"Score:\s*(\d+)", result_text)
            score = int(score_match.group(1)) if score_match else 0

            return {"score": score, "comment": result_text}
        except Exception as e:
            print(f"[ERROR] GPT 평가 실패: {e}")
            return {"score": 0, "comment": "Error during evaluation"}


        
    def _save_results(self):
        with open(os.path.join(self.results_dir, "highlv_results.json"), "w", encoding="utf-8") as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        with open(os.path.join(self.results_dir, "no_py_generated.json"), "w", encoding="utf-8") as f:
            json.dump(self.no_py_generated, f, indent=2, ensure_ascii=False)
        with open(os.path.join(self.results_dir, "no_pdf_generated.json"), "w", encoding="utf-8") as f:
            json.dump(self.no_pdf_generated, f, indent=2, ensure_ascii=False)
        with open(os.path.join(self.results_dir, "corrupted_pdfs.json"), "w", encoding="utf-8") as f:  # ✅ 손상된 PDF 저장
            json.dump(self.corrupted_pdf_files, f, indent=2, ensure_ascii=False)
        
if __name__ == "__main__":
    evaluator = Evaluator(
        original_dataset_dir="/root/SOJUNG_STUFF/ChartMimic/dataset/direct_600",
        generated_dataset_dir="/root/SOJUNG_STUFF/ChartMimic/results/direct/chart2code_deepseek-vl-7b-chat_DirectAgent_results/direct_checker",
        results_dir="/root/SOJUNG_STUFF/ChartMimic/highlv_eval/deepseek-vl-7b-chat_gpt-4o"
    )